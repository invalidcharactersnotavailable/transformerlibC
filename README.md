# TransformerLibC - Pure C Transformer Implementation

A complete, production-ready transformer implementation in pure C with dynamic memory allocation, comprehensive training capabilities, and inference support.

## üöÄ Features

### Core Architecture
- **Pure C Implementation**: No external dependencies beyond standard library
- **Dynamic Memory Allocation**: All tensors, weights, and activations dynamically allocated
- **Mixed Precision**: Support for float32 and float16 operations
- **Efficient Autodiff**: Automatic differentiation with memory-efficient backpropagation
- **Optimized Operations**: SIMD-optimized matrix operations (AVX/AVX2)

### Training Capabilities
- **CLI Training**: Full training pipeline accessible via command line
- **Folder-based Data Loading**: Train on entire directories of text files
- **JSON Finetuning**: Support for HuggingFace-style JSON datasets
- **Checkpointing**: Save/load model states with full parameter persistence
- **Gradient Accumulation**: Support for large effective batch sizes
- **Multiple Optimizers**: SGD and Adam with configurable hyperparameters

### Inference & Model Support
- **Universal Model Loading**: Load and run inference on any compatible model
- **Flexible Input Formats**: Support for various input tokenization schemes
- **Real-time Generation**: Stream-based text generation with configurable parameters
- **Memory-efficient Inference**: Optimized for deployment scenarios

### Data Processing
- **Corpus Creation**: Automatic vocabulary building from training data
- **Dynamic Tokenization**: Character-level and subword tokenization support
- **Data Augmentation**: Built-in data preprocessing and augmentation
- **Multi-format Support**: Text files, JSON datasets, and custom formats

## üì¶ Installation

### Prerequisites
```bash
# Ubuntu/Debian
sudo apt-get install gcc make libc6-dev

# CentOS/RHEL
sudo yum install gcc make glibc-devel

# macOS
brew install gcc make
```

### Build
```bash
# Basic build
make

# With BLAS acceleration (recommended for performance)
make USE_CBLAS=1

# Release build (optimized)
make release
```

## üéØ Quick Start

### Basic Training
```bash
# Train on a text file
./transformer_test --data data.txt --save model.ckpt --epochs 10

# Train on a directory of text files
./transformer_test --data_dir ./corpus/ --save model.ckpt --epochs 20

# Load and continue training
./transformer_test --load model.ckpt --data data.txt --save model_new.ckpt
```

### Finetuning
```bash
# Finetune on JSON dataset (HuggingFace format)
./transformer_test --load base_model.ckpt --finetune_dir ./finetune_data/ --save finetuned.ckpt

# JSON format example (finetune_data/sample.json):
# {"input": "Translate to French:", "output": "Traduire en fran√ßais:"}
```

### Inference
```bash
# Generate text from loaded model
./transformer_test --load model.ckpt --inference --prompt "Hello world" --max_length 50

# Interactive generation
./transformer_test --load model.ckpt --interactive
```

## üîß Configuration

### Command Line Options

#### Model Architecture
- `--n_layers <int>`: Number of transformer layers (default: 2)
- `--embed_dim <int>`: Embedding dimension (default: 512)
- `--n_heads <int>`: Number of attention heads (default: 8)
- `--ff_hidden_dim <int>`: Feed-forward hidden dimension (default: 2048)
- `--vocab_size <int>`: Vocabulary size (default: 10000)
- `--max_seq_len <int>`: Maximum sequence length (default: 512)

#### Training Parameters
- `--batch_size <int>`: Batch size (default: 1)
- `--seq_len <int>`: Training sequence length (default: 10)
- `--learning_rate <float>`: Learning rate (default: 0.001)
- `--epochs <int>`: Number of training epochs (default: 10)
- `--gradient_accumulation_steps <int>`: Gradient accumulation steps (default: 4)
- `--optimizer <string>`: Optimizer type: "sgd" or "adam" (default: "adam")

#### Data & I/O
- `--data <file>`: Single text file for training
- `--data_dir <dir>`: Directory containing text files for training
- `--finetune_dir <dir>`: Directory with JSON files for finetuning
- `--save <file>`: Save model checkpoint
- `--load <file>`: Load model checkpoint
- `--vocab_file <file>`: Custom vocabulary file

#### Inference Options
- `--inference`: Enable inference mode
- `--prompt <text>`: Input prompt for generation
- `--max_length <int>`: Maximum generation length
- `--temperature <float>`: Sampling temperature (default: 1.0)
- `--top_k <int>`: Top-k sampling (default: 40)
- `--interactive`: Interactive generation mode

#### System Options
- `--test`: Run performance test and exit
- `--verbose`: Enable verbose logging
- `--seed <int>`: Random seed for reproducibility

### Environment Variables
```bash
# Memory management
export ARENA_SIZE_MB=4096  # Training arena size in MB
export INFERENCE_ARENA_MB=1024  # Inference arena size in MB

# Performance tuning
export OMP_NUM_THREADS=4  # OpenMP thread count
export USE_BLAS=1  # Enable BLAS acceleration
```

## üìä Architecture Overview

### Core Components

#### Tensor System
```c
typedef struct {
    int n_dims;
    int *dims;
    void *data;
    void *grad;
    DataType dtype;
} Tensor;
```
- Dynamic allocation with automatic memory management
- Support for float32, float16, and int32 data types
- Efficient gradient computation for autodiff

#### Transformer Architecture
```c
typedef struct {
    int n_layers;
    int vocab_size;
    int max_seq_len;
    int embed_dim;
    int n_heads;
    int ff_hidden_dim;
    LayerNorm* ln1;
    LayerNorm* ln2;
    Embedding* token_embedding;
    Embedding* pos_embedding;
    EncoderLayer* layers;
} Transformer;
```

#### Memory Management
- **Arena Allocator**: Fast temporary allocations for training
- **Dynamic Allocation**: All model weights and activations dynamically allocated
- **Memory Pooling**: Efficient reuse of memory during training
- **Garbage Collection**: Automatic cleanup of intermediate tensors

### Training Pipeline

1. **Data Loading**: Read text files or JSON datasets
2. **Tokenization**: Convert text to token IDs
3. **Batching**: Create mini-batches with padding
4. **Forward Pass**: Compute logits through transformer
5. **Loss Computation**: Calculate cross-entropy loss
6. **Backward Pass**: Compute gradients via autodiff
7. **Optimization**: Update weights using SGD/Adam
8. **Checkpointing**: Save model state periodically

### Inference Pipeline

1. **Model Loading**: Load trained model from checkpoint
2. **Tokenization**: Convert input prompt to tokens
3. **Generation**: Autoregressive text generation
4. **Sampling**: Apply temperature, top-k, or greedy decoding
5. **Output**: Convert tokens back to text

## üìà Performance

### Benchmarks
- **Training**: ~1000 tokens/second on CPU (varies by model size)
- **Inference**: ~2000 tokens/second on CPU
- **Memory**: ~2GB for 100M parameter model
- **Scalability**: Supports models up to 1B+ parameters

### Optimization Features
- SIMD-optimized matrix operations (AVX/AVX2)
- BLAS integration for accelerated linear algebra
- OpenMP parallelization for multi-core systems
- Memory-efficient gradient computation
- Optimized attention mechanism

## üîç Debugging & Troubleshooting

### Common Issues

#### Segmentation Faults
```bash
# Enable debug symbols
make clean && make

# Run with gdb
gdb ./transformer_test
(gdb) run --data test.txt --test

# Check memory usage
valgrind --leak-check=full ./transformer_test --data test.txt
```

#### Memory Issues
```bash
# Increase arena size
export ARENA_SIZE_MB=8192
./transformer_test --data large_file.txt

# Monitor memory usage
top -p $(pgrep transformer_test)
```

#### Performance Issues
```bash
# Enable BLAS acceleration
make USE_CBLAS=1
./transformer_test --data data.txt

# Profile with perf
perf record ./transformer_test --data data.txt
perf report
```

### Logging
```bash
# Enable verbose logging
./transformer_test --verbose --data data.txt

# Log levels: INFO, WARN, ERROR, DEBUG
export LOG_LEVEL=DEBUG
```


## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Inspired by the original "Attention Is All You Need" paper
- Built on efficient C programming practices
- Optimized for educational and production use

## üìû Support

For issues, questions, or contributions:
- Open an issue on GitHub
- Check the troubleshooting section
- Review the examples and documentation

---

**Note**: This is a production-ready implementation suitable for both educational purposes and real-world applications. The codebase is actively maintained and optimized for performance and reliability.
